{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p /root/.pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/tf/backup/backup': File exists\n",
      "ln: failed to create symbolic link '/tf/submission/submission': File exists\n"
     ]
    }
   ],
   "source": [
    "! /mnt/utils/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/.pip/pip.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile /root/.pip/pip.conf\n",
    "[global]\n",
    "index-url=http://ftp.daumkakao.com/pypi/simple\n",
    "trusted-host=ftp.daumkakao.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import pytz\n",
    "import random\n",
    "import shutil\n",
    "# import splitfolders\n",
    "import copy\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "# import pydicom as pdm\n",
    "import pickle\n",
    "import yaml\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import albumentations as albu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchtuples as tt\n",
    "from sklearn import metrics\n",
    "\n",
    "import torchvision\n",
    "import timm \n",
    "# import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.8/dist-packages (0.4.12)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.8/dist-packages (from timm) (1.10.0+cu113)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.11.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (8.3.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorchversion: 1.10.0+cu113 Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else: DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorchversion:', torch.__version__, 'Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/ainode/dataset/train/'\n",
    "case_lst = [path.split('/')[-1].split('_')[0] for path in glob(DATA_DIR+'flow/*')]\n",
    "case_lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_Number</th>\n",
       "      <th>Event_Label</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Start_Epoch</th>\n",
       "      <th>End_Epoch</th>\n",
       "      <th>Duration(second)</th>\n",
       "      <th>FPS</th>\n",
       "      <th>Frames</th>\n",
       "      <th>Case</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wake</td>\n",
       "      <td>2020/05/02 22:20:30.000</td>\n",
       "      <td>2020/05/02 22:21:00.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0, 150]</td>\n",
       "      <td>A2020-EM-01-0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Wake</td>\n",
       "      <td>2020/05/02 22:21:00.000</td>\n",
       "      <td>2020/05/02 22:21:30.000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[150, 300]</td>\n",
       "      <td>A2020-EM-01-0001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Wake</td>\n",
       "      <td>2020/05/02 22:21:30.000</td>\n",
       "      <td>2020/05/02 22:22:00.000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[300, 450]</td>\n",
       "      <td>A2020-EM-01-0001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Wake</td>\n",
       "      <td>2020/05/02 22:22:00.000</td>\n",
       "      <td>2020/05/02 22:22:30.000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[450, 600]</td>\n",
       "      <td>A2020-EM-01-0001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Wake</td>\n",
       "      <td>2020/05/02 22:22:30.000</td>\n",
       "      <td>2020/05/02 22:23:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[600, 750]</td>\n",
       "      <td>A2020-EM-01-0001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Event_Number Event_Label               Start_Time                 End_Time  \\\n",
       "0             0        Wake  2020/05/02 22:20:30.000  2020/05/02 22:21:00.000   \n",
       "1             4        Wake  2020/05/02 22:21:00.000  2020/05/02 22:21:30.000   \n",
       "2             5        Wake  2020/05/02 22:21:30.000  2020/05/02 22:22:00.000   \n",
       "3             7        Wake  2020/05/02 22:22:00.000  2020/05/02 22:22:30.000   \n",
       "4             8        Wake  2020/05/02 22:22:30.000  2020/05/02 22:23:00.000   \n",
       "\n",
       "   Start_Epoch  End_Epoch  Duration(second)  FPS      Frames  \\\n",
       "0            1          2              30.0  5.0    [0, 150]   \n",
       "1            2          3              30.0  5.0  [150, 300]   \n",
       "2            3          4              30.0  5.0  [300, 450]   \n",
       "3            4          5              30.0  5.0  [450, 600]   \n",
       "4            5          6              30.0  5.0  [600, 750]   \n",
       "\n",
       "               Case  ID  \n",
       "0  A2020-EM-01-0001   0  \n",
       "1  A2020-EM-01-0001   1  \n",
       "2  A2020-EM-01-0001   2  \n",
       "3  A2020-EM-01-0001   3  \n",
       "4  A2020-EM-01-0001   4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_annot_df = pd.read_csv(f'./DATA/annot.csv')\n",
    "p_annot_df['ID'] = list(range(len(p_annot_df)))\n",
    "p_annot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = '/ainode/dataset/train/'\n",
    "FLOW_DIR = '/ainode/dataset/train/flow/'\n",
    "RGB_DIR = '/ainode/dataset/train/rgb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Light Sleep    609\n",
       "Deep Sleep     245\n",
       "Wake           146\n",
       "Name: Event_Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_annot_df['Event_Label'].iloc[1200:2200].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Val Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = set([path.split('/')[-1].split('_')[0] for path in glob('./average_images33/*')])\n",
    "cases = list(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A2021-EM-01-0080',\n",
       " 'A2020-EM-01-0003',\n",
       " 'A2020-EM-01-0049',\n",
       " 'A2021-EM-01-0117',\n",
       " 'A2020-EM-01-0061']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincases = cases[:35]\n",
    "valcases = cases[35:41]\n",
    "testcases = cases[41:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = traincases, valcases, testcases\n",
    "\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = []\n",
    "for case in traincases:\n",
    "    y_train.append(le.fit_transform(p_annot_df[p_annot_df['Case']==case]['Event_Label'].tolist()))\n",
    "    \n",
    "y_val = []\n",
    "for case in valcases:\n",
    "    y_val.append(le.fit_transform(p_annot_df[p_annot_df['Case']==case]['Event_Label'].tolist()))\n",
    "    \n",
    "y_test = []\n",
    "for case in testcases:\n",
    "    y_test.append(le.fit_transform(p_annot_df[p_annot_df['Case']==case]['Event_Label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1, 2]), array([313, 321, 148])),\n",
       " (array([0, 1, 2]), array([138, 321, 349])),\n",
       " (array([0, 1, 2]), array([238, 498, 101])))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train[0], return_counts=True),\\\n",
    "np.unique(y_val[0], return_counts=True),\\\n",
    "np.unique(y_test[0], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self,  \n",
    "            X,\n",
    "            y,\n",
    "            imgtype, # 'rgb','flow_x','flow_y'\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.cases = X\n",
    "        self.imgtype = imgtype\n",
    "        self.levels = y\n",
    "        self.labels = [2,1,0]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        case = self.cases[i]\n",
    "        imgs_path = f'./average_images33/{case}_'\n",
    "        img_lst = glob(imgs_path+'*.jpg')\n",
    "        img_lst = sorted(img_lst, key=lambda img: img.split('_')[-1].split('.')[0].zfill(4))\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        for idx, path in enumerate(img_lst):\n",
    "            image = np.array(Image.open(path).resize((128,128)))\n",
    "            image = np.transpose(image, (2, 1, 0))\n",
    "            images.append(image)\n",
    "\n",
    "        images = np.array(images)\n",
    "#         images = np.transpose(images, (1,0,2,3))\n",
    "        images = torch.Tensor(images)\n",
    "        \n",
    "        level = torch.as_tensor(self.levels[i])\n",
    "#         level = F.one_hot(level, num_classes=3)\n",
    "        \n",
    "        \n",
    "#         target = np.full(shape=len(level), fill_value = 1, dtype = np.int)\n",
    "#         target = target[:,np.newaxis]\n",
    "#         target = torch.Tensor(target)\n",
    "        \n",
    "#         label = torch.as_tensor(self.labels)\n",
    "\n",
    "#         if self.preprocessing:\n",
    "#             sample = self.preprocessing(images = images)\n",
    "#             images = sample['images']\n",
    "        \n",
    "        return images, level #, label\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x7FD270D8AEE0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset(X_train, y_train, 'rgb')\n",
    "#                   augmentation = get_validation_augmentation()) #, preprocessing = get_preprocessing())\n",
    "# dataset.levels\n",
    "\n",
    "image, level = dataset[0] #, label\n",
    "\n",
    "# np.array(image[0]).shape\n",
    "# dataset.cases, dataset.epochs\n",
    "# np.unique(np.array(images[6]))\n",
    "# Image.fromarray(np.array(image).astype('uint8'))\n",
    "Image.fromarray(np.transpose(np.array(image[0]).astype('uint8')))\n",
    "# image.shape #torch.Size([600, 3, 128, 128])\n",
    "# target.shape #torch.Size([600])\n",
    "# level.shape #torch.Size([600,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagetype = 'rgb' # 'rgb', 'flow_x','flow_y'\n",
    "labels = ['Wake', 'Light Sleep','Deep Sleep']\n",
    "train_dataset = Dataset(X_train, y_train, imagetype, labels)\n",
    "val_dataset = Dataset(X_val, y_val, imagetype, labels)\n",
    "test_dataset = Dataset(X_test, y_test, imagetype, labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "Xtrain: torch.Size([1, 942, 3, 128, 128]) type: torch.FloatTensor\n",
      "ytrain: torch.Size([1, 942]) type: torch.LongTensor\n",
      "\n",
      "6\n",
      "Xval: torch.Size([1, 857, 3, 128, 128]) type: torch.FloatTensor\n",
      "yval: torch.Size([1, 857]) type: torch.LongTensor\n",
      "\n",
      "6\n",
      "Xtest: torch.Size([1, 861, 3, 128, 128]) type: torch.FloatTensor\n",
      "ytest: torch.Size([1, 861]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "\n",
    "Xtrain, ytrain = next(iter(train_loader))\n",
    "print('Xtrain:', Xtrain.size(), 'type:', Xtrain.type())\n",
    "# print('ttrain:', ttrain.size(), 'type:', ttrain.type())\n",
    "print('ytrain:', ytrain.size(), 'type:', ytrain.type())\n",
    "\n",
    "print()\n",
    "print(len(val_loader))\n",
    "\n",
    "Xval, yval = next(iter(val_loader))\n",
    "print('Xval:', Xval.size(), 'type:', Xval.type())\n",
    "# print('tval:', tval.size(), 'type:', tval.type())\n",
    "print('yval:', yval.size(), 'type:', yval.type())\n",
    "\n",
    "print()\n",
    "print(len(test_loader))\n",
    "\n",
    "Xtest, ytest = next(iter(test_loader))\n",
    "print('Xtest:', Xtest.size(), 'type:', Xtest.type())\n",
    "# print('ttest:', ttest.size(), 'type:', ttest.type())\n",
    "print('ytest:', ytest.size(), 'type:', ytest.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.27 ms, sys: 115 ms, total: 122 ms\n",
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xtrain, ytrain= next(iter(train_loader)) #, ltrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
       "         2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1,\n",
       "         1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0,\n",
       "         0, 0, 0, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2,\n",
       "         1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2,\n",
       "         1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
       "         2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pltsize = 4                         \n",
    "\n",
    "# plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "# for i in range(len(Xtrain[:10])):\n",
    "#     plt.subplot(1, 10, i+1)\n",
    "#     plt.axis('off')\n",
    "#     img = np.transpose(Xtrain.numpy()[i][:,0,:,:], (2,1,0))[:,:,0]\n",
    "#     plt.imshow(img,cmap=\"gray_r\")\n",
    "# #     plt.title('Class: ' + str(y_train[i].item()))\n",
    "#     plt.title('Class: ' + str(ytrain[i])[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Requirement already satisfied: torch==1.10.0+cu113 in /usr/local/lib/python3.8/dist-packages (1.10.0+cu113)\n",
      "Requirement already satisfied: torchvision==0.11.1+cu113 in /usr/local/lib/python3.8/dist-packages (0.11.1+cu113)\n",
      "Requirement already satisfied: torchaudio==0.10.0+cu113 in /usr/local/lib/python3.8/dist-packages (0.10.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.0+cu113) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.1+cu113) (1.19.5)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.1+cu113) (8.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  detection\t\t    mnasnet.py\t    regnet.py\t     vgg.py\n",
      "__pycache__  efficientnet.py\t    mobilenet.py    resnet.py\t     video\n",
      "_utils.py    feature_extraction.py  mobilenetv2.py  segmentation\n",
      "alexnet.py   googlenet.py\t    mobilenetv3.py  shufflenetv2.py\n",
      "densenet.py  inception.py\t    quantization    squeezenet.py\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/local/lib/python3.8/dist-packages/torchvision/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.move('./efficientnet.py', '/usr/local/lib/python3.8/dist-packages/torchvision/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class efficientnetb0:\n",
    "    model_name=\"efficientnetb0\"\n",
    "    batch_size = 16\n",
    "    WORKERS = 4\n",
    "    classes = 9\n",
    "    epochs = 30\n",
    "    optimizer = \"torch.optim.AdamW\"\n",
    "    optimizer_parm = {'lr':1e-3,'weight_decay':0.00001}\n",
    "    scheduler = \"torch.optim.lr_scheduler.CosineAnnealingLR\"\n",
    "    scheduler_parm = {'T_max':5500,'eta_min':0.000001}\n",
    "    loss_fn = 'torch.nn.CrossEntropyLoss'\n",
    "    MODEL_PATH = 'log/cpt'\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = efficientnetb0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): ConvNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): ConvNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): ConvNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=True)\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=3, bias=True)\n",
       "        (1): Softmax(dim=None)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "# cnn = torchvision.models.densenet121(pretrained = True)\n",
    "cnn = torchvision.models.efficientnet_b0(pretrained=True).cuda()\n",
    "\n",
    "num_ftrs = cnn.classifier[1].in_features\n",
    "cnn.classifier[1] = nn.Linear(num_ftrs,3)\n",
    "cnn.classifier[1] = nn.Sequential(cnn.classifier[1], nn.Softmax(),)\n",
    "\n",
    "cnn = torch.nn.DataParallel(cnn)\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                          Size  Used Avail Use% Mounted on\n",
      "overlay                              80G   30G   51G  38% /\n",
      "tmpfs                                64M     0   64M   0% /dev\n",
      "tmpfs                                94G     0   94G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1                       80G   30G   51G  38% /mnt\n",
      "/efs/team/378/backup                8.0E  2.9T  8.0E   1% /mnt/backup\n",
      "/efs/team/378/submission/submitted  8.0E  2.9T  8.0E   1% /mnt/submission/submitted\n",
      "/efs/team/378/submission/submit     8.0E  2.9T  8.0E   1% /mnt/submission/submit\n",
      "/efs/dataset/9                      8.0E  2.9T  8.0E   1% /mnt/dataset\n",
      "tmpfs                                94G  338M   93G   1% /dev/shm\n",
      "/dev/nvme3n1                        3.0T  1.4T  1.6T  47% /ainode/dataset\n",
      "tmpfs                                94G   12K   94G   1% /run/secrets/kubernetes.io/serviceaccount\n",
      "tmpfs                                94G   12K   94G   1% /proc/driver/nvidia\n",
      "devtmpfs                             94G     0   94G   0% /dev/nvidia0\n",
      "tmpfs                                94G     0   94G   0% /proc/acpi\n",
      "tmpfs                                94G     0   94G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,cnn, embed_size=1280, LSTM_UNITS=64, DO = 0.3):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.cnn = cnn.module\n",
    "        self.cnn.eval().cuda()\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(LSTM_UNITS*2, LSTM_UNITS*2)\n",
    "        self.linear2 = nn.Linear(LSTM_UNITS*2, LSTM_UNITS*2)\n",
    "\n",
    "#         self.linear_pe = nn.Linear(LSTM_UNITS*2, 1)\n",
    "        self.linear_global = nn.Linear(LSTM_UNITS*2, 3)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        with torch.no_grad():\n",
    "            embedding = self.cnn.features(x) #extract_features(x)\n",
    "            embedding = self.avgpool(embedding)\n",
    "            b,f,_,_ = embedding.shape\n",
    "            embedding = embedding.reshape(1,b,f)\n",
    "        self.lstm1.flatten_parameters()\n",
    "        h_lstm1, _ = self.lstm1(embedding)\n",
    "        self.lstm2.flatten_parameters()\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        h_conc_linear1  = F.relu(self.linear1(h_lstm1))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_lstm2))\n",
    "        \n",
    "        hidden = h_lstm1 + h_lstm2 + h_conc_linear1 + h_conc_linear2\n",
    "\n",
    "#         output = self.linear_pe(hidden)\n",
    "#         print(\"shape of hidden:\", hidden.shape)\n",
    "    \n",
    "        hid = torch.unsqueeze(hidden[0][0],0)\n",
    "        out = self.linear_global(hid)\n",
    "        output_global = out\n",
    "        \n",
    "        for hid in hidden[0][1:]:\n",
    "#             print('hid.shape:',hid.shape)\n",
    "            hid = torch.unsqueeze(hid, 0)\n",
    "            out = self.linear_global(hid)\n",
    "            output_global = torch.cat((output_global, out), 0)\n",
    "            \n",
    "        output_global = torch.unsqueeze(output_global,0)\n",
    "            \n",
    "#         print('룰루')\n",
    "#         print(output_global.shape)\n",
    "#         output_global = torch.Tensor(output_global)\n",
    "    \n",
    "        return output_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(cnn).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = eval(model_config.optimizer)(model.parameters(),**model_config.optimizer_parm)\n",
    "scheduler = eval(model_config.scheduler)(optimizer,**model_config.scheduler_parm)\n",
    "loss_fn = eval(model_config.loss_fn)()#pos_weight=torch.FloatTensor(model_config.pos_weight).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (cnn): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): ConvNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): ConvNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): ConvNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): ConvNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): ConvNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=True)\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=3, bias=True)\n",
       "        (1): Softmax(dim=None)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (lstm1): LSTM(1280, 64, batch_first=True, bidirectional=True)\n",
       "  (lstm2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (linear_global): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "class trainer:\n",
    "    def __init__(self,loss_fn,model,optimizer,scheduler,config):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.config = config\n",
    "\n",
    "        \n",
    "    def batch_train(self, batch_imgs, batch_labels):\n",
    "        imgs = batch_imgs.to(DEVICE)\n",
    "        labels = batch_labels.to(DEVICE)\n",
    "        self.optimizer.zero_grad()\n",
    "        predicted = self.model(imgs)\n",
    "#         predicted = torch.Tensor(predicted)\n",
    "        loss = 0\n",
    "        for i in range(len(labels[0])-1):\n",
    "            loss += criterion(predicted[:,i], labels[:,i])\n",
    "        loss /= len(labels[0])-1 \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        prediction = predicted.max(1, keepdim=True)[1]\n",
    "        \n",
    "#         train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/BATCH_SIZE)\n",
    "        \n",
    "        return loss, predicted\n",
    "    \n",
    "    def batch_valid(self, batch_imgs,get_fet):\n",
    "        self.model.eval()\n",
    "        batch_imgs = batch_imgs.cuda()\n",
    "        with torch.no_grad():\n",
    "            predicted = self.model(batch_imgs)\n",
    "            prediction = []\n",
    "            for pred in predicted:\n",
    "                pred = pred.absolute()\n",
    "                prediction.append(pred.max(1, keepdim=True)[1])\n",
    "#             predicted[0] = torch.sigmoid(predicted[0])\n",
    "#             predicted[1] = torch.sigmoid(predicted[1])\n",
    "#         return prediction\n",
    "        return prediction, predicted\n",
    "    \n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        train_loss = AverageMeter()\n",
    "        train_acc = AverageMeter()\n",
    "        for batch_idx, (imgs,labels) in enumerate(tqdm_loader):\n",
    "#             print(imgs.shape)\n",
    "            loss, predicted = self.batch_train(imgs[0], labels)\n",
    "            current_loss_mean = (current_loss_mean * batch_idx + loss) / (batch_idx + 1)\n",
    "            tqdm_loader.set_description('loss: {:.4} lr:{:.6}'.format(\n",
    "                    current_loss_mean, self.optimizer.param_groups[0]['lr']))\n",
    "            self.scheduler.step(batch_idx)\n",
    "#             if batch_idx>10:\n",
    "#                 break\n",
    "        return current_loss_mean\n",
    "    \n",
    "    def valid_epoch(self, loader,name=\"valid\"):\n",
    "        self.model.eval()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        correct = 0\n",
    "        val_loss = AverageMeter()\n",
    "        val_acc = AverageMeter()\n",
    "        for batch_idx, (imgs,labels) in enumerate(tqdm_loader):\n",
    "            with torch.no_grad():\n",
    "                imgs = imgs[0].to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                predicted = self.model(imgs)\n",
    "                loss = 0\n",
    "                for i in range(len(labels[0])-1):\n",
    "                    loss += criterion(predicted[:,i], labels[:,i])\n",
    "                loss /= len(labels[0])-1 \n",
    "                prediction = predicted.max(1, keepdim=True)[1]\n",
    "                \n",
    "#                 val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/BATCH_SIZE)\n",
    "                \n",
    "                current_loss_mean = (current_loss_mean * batch_idx + loss) / (batch_idx + 1)\n",
    "                tqdm_loader.set_description(f\"loss : {current_loss_mean:.4}\")\n",
    "#             if batch_idx>10:\n",
    "#                 break\n",
    "        score = 1-current_loss_mean\n",
    "        print('metric {}'.format(score))\n",
    "        return score\n",
    "    \n",
    "    def run(self,train_loder,val_loder):\n",
    "        best_score = -100000\n",
    "        for e in range(self.config.epochs):\n",
    "            print(\"----------Epoch {}-----------\".format(e))\n",
    "            current_loss_mean = self.train_epoch(train_loder)\n",
    "            score = self.valid_epoch(val_loder)\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(self.model.state_dict(),self.config.MODEL_PATH+\"/{}_best.pth\".format(self.config.model_name))\n",
    "                print(\"==================best model saved===================\")\n",
    "\n",
    "    def batch_valid_tta(self, batch_imgs):\n",
    "        batch_imgs = batch_imgs.cuda()\n",
    "        predicted = model(batch_imgs)\n",
    "        tta_flip = [[-1],[-2]]\n",
    "        for axis in tta_flip:\n",
    "            predicted += torch.flip(model(torch.flip(batch_imgs, axis)), axis)\n",
    "        predicted = predicted/(1+len(tta_flip))\n",
    "        predicted = torch.sigmoid(predicted)\n",
    "        return predicted.cpu().numpy()\n",
    "            \n",
    "    def load_best_model(self):\n",
    "        if os.path.exists(self.config.MODEL_PATH+\"/{}_best.pth\".format(self.config.model_name)):\n",
    "            self.model.load_state_dict(torch.load(self.config.MODEL_PATH+\"/{}_best.pth\".format(self.config.model_name)))\n",
    "            print(\"load best model\")\n",
    "        \n",
    "    def predict(self,imgs_tensor,get_fet = False):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.batch_valid(imgs_tensor,get_fet=get_fet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = trainer(loss_fn,model,optimizer,scheduler,config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=model_config.WORKERS, pin_memory=False)\n",
    "val = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=model_config.WORKERS, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 0-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44da2b5fc19f45f3aa12553d0e9aa089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42f6f3436a24ad18055c2eef2f46e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.01955437660217285\n",
      "==================best model saved===================\n",
      "----------Epoch 1-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f04ba027204cf887a7ebb0d1cefc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e182ad07ad7240df983e1c56eede0b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006775498390197754\n",
      "==================best model saved===================\n",
      "----------Epoch 2-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339f6c728b3b46fd9ef27cada8b24fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1a87cfae6849148f943ef8555edf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006103396415710449\n",
      "==================best model saved===================\n",
      "----------Epoch 3-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424809411a944ca78fd5c6d82386c116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba58664f6a44ba7ae7cf8e190f04a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006200313568115234\n",
      "----------Epoch 4-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6839e3aec0fd4d33974e95382278e79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272e2a5605fc4d55b89ff72c53b63108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.00622868537902832\n",
      "----------Epoch 5-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722c3ba153a4407e8c2205237ee987ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ec7f4792de45d19b8f2020ee6c7490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006157636642456055\n",
      "----------Epoch 6-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ee83cfefbb4b759d900ef38b4dec71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab524ce3e8564158ad587210e3c18c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    if w.is_alive():\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    if w.is_alive():\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError: can only test a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    if w.is_alive():\n",
      "AssertionError: can only test a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Traceback (most recent call last):\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    if w.is_alive():\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    if w.is_alive():\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.005975604057312012\n",
      "==================best model saved===================\n",
      "----------Epoch 7-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dbb8bb380b42c39abefd6bf456733f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6618e1ee8ffa48809eae5b30fb98c983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006007671356201172\n",
      "----------Epoch 8-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1852b2bdaf6f47c08b603d2ee85bbdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375b775e89bc4d2da3fabc8a6216ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006193995475769043\n",
      "----------Epoch 9-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9be840c7e244afbeb60609bfb187a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd94b0c4933b4247a35ccd3381342ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.0062988996505737305\n",
      "----------Epoch 10-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69302f37c7bf4a9f8f91bc41c4e80935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    if w.is_alive():\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "AssertionError: can only test a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    if w.is_alive():\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "    if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    if w.is_alive():\n",
      "AssertionError: can only test a child process\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fea20cdf280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db75587253af4c7da486710ad46fd9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006216883659362793\n",
      "----------Epoch 11-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f71f30a6bc40b492d89578af5a7078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51241ddd47b419e8f046b4c150b6ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.00610959529876709\n",
      "----------Epoch 12-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc5fbdda4654461b5381384a6ba918f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00916d02be048bea37b54d28d9e8214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.0061533451080322266\n",
      "----------Epoch 13-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfc374063244f299428c6074fb7e910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f2c970d0904f87bece475279b8ee3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.0060929059982299805\n",
      "----------Epoch 14-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0d002455394493a3e5def71492ff8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505e71c87fd4450ca08e71231fb71335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006121158599853516\n",
      "----------Epoch 15-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ae2766ea27432dbca6d235df40d0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc28aa47821407a8a8159ead9c49b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric -0.006032705307006836\n",
      "----------Epoch 16-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6a9725eaf24abdbea6c41286cd984a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f90c520a61047b5b1631ebdea2dfe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Trainer.run(train,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best model\n"
     ]
    }
   ],
   "source": [
    "Trainer.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997cebbe01464c98a6984fa2607c2278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = DataLoader(test_dataset)\n",
    "\n",
    "inference = []\n",
    "for image, level in tqdm(test):\n",
    "    inference.append(Trainer.predict(image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.2234,  0.4962, -0.2866], device='cuda:0'),\n",
       " tensor([-0.2294,  0.5237, -0.2962], device='cuda:0'),\n",
       " tensor([-0.2345,  0.5381, -0.3015], device='cuda:0'),\n",
       " tensor([-0.2382,  0.5461, -0.3046], device='cuda:0'),\n",
       " tensor([-0.2406,  0.5506, -0.3063], device='cuda:0'),\n",
       " tensor([-0.2421,  0.5531, -0.3073], device='cuda:0'),\n",
       " tensor([-0.2430,  0.5545, -0.3078], device='cuda:0'),\n",
       " tensor([-0.2434,  0.5553, -0.3081], device='cuda:0'),\n",
       " tensor([-0.2437,  0.5557, -0.3083], device='cuda:0'),\n",
       " tensor([-0.2439,  0.5559, -0.3083], device='cuda:0'),\n",
       " tensor([-0.2439,  0.5560, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5560, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2440,  0.5561, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2441,  0.5560, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2441,  0.5560, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2441,  0.5559, -0.3084], device='cuda:0'),\n",
       " tensor([-0.2442,  0.5558, -0.3083], device='cuda:0'),\n",
       " tensor([-0.2443,  0.5556, -0.3083], device='cuda:0'),\n",
       " tensor([-0.2444,  0.5553, -0.3081], device='cuda:0'),\n",
       " tensor([-0.2446,  0.5548, -0.3078], device='cuda:0'),\n",
       " tensor([-0.2448,  0.5538, -0.3072], device='cuda:0'),\n",
       " tensor([-0.2451,  0.5523, -0.3061], device='cuda:0'),\n",
       " tensor([-0.2454,  0.5495, -0.3039], device='cuda:0'),\n",
       " tensor([-0.2455,  0.5448, -0.2997], device='cuda:0'),\n",
       " tensor([-0.2452,  0.5361, -0.2916], device='cuda:0'),\n",
       " tensor([-0.2438,  0.5176, -0.2758], device='cuda:0'),\n",
       " tensor([-0.2412,  0.4710, -0.2400], device='cuda:0')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inference[0][1][0])[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]], device='cuda:0')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-885660be2d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "inference[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(inference[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "class trainer:\n",
    "    def __init__(self,loss_fn,model,optimizer,scheduler,config):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.config = config\n",
    "\n",
    "        \n",
    "    def batch_train(self, batch_imgs, batch_labels0, batch_idx): #,batch_labels1\n",
    "        batch_imgs, batch_labels0 = batch_imgs.cuda().float(), batch_labels0.cuda().float() #,batch_labels1 = ,batch_labels1.cuda().floa\n",
    "        predicted = self.model(batch_imgs)\n",
    "        loss1 = self.loss_fn(predicted.float().reshape(-1), batch_labels0.reshape(-1))\n",
    "#         loss2 = self.loss_fn(predicted[1].float().reshape(-1), batch_labels1.reshape(-1))\n",
    "        loss = loss1 #+loss2\n",
    "#         loss = loss/2.0\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss.item(), predicted\n",
    "    \n",
    "    def batch_valid(self, batch_imgs,get_fet):\n",
    "        self.model.eval()\n",
    "        batch_imgs = batch_imgs.cuda()\n",
    "        with torch.no_grad():\n",
    "            predicted = self.model(batch_imgs)\n",
    "            print(predicted)\n",
    "#             predicted[0] = torch.sigmoid(predicted[0])\n",
    "#             predicted[1] = torch.sigmoid(predicted[1])\n",
    "#             prediction = np.array(predicted).max(0) #, keepdim = True)[1] \n",
    "        return prediction\n",
    "    \n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        for batch_idx, (imgs,labels) in enumerate(tqdm_loader): #,labels1)\n",
    "            loss, predicted = self.batch_train(imgs[0], labels, batch_idx) #labels1\n",
    "            current_loss_mean = (current_loss_mean * batch_idx + loss) / (batch_idx + 1)\n",
    "            tqdm_loader.set_description('loss: {:.4} lr:{:.6}'.format(\n",
    "                    current_loss_mean, self.optimizer.param_groups[0]['lr']))\n",
    "            self.scheduler.step(batch_idx)\n",
    "            if batch_idx>10:\n",
    "                break\n",
    "        return current_loss_mean\n",
    "    \n",
    "    def valid_epoch(self, loader,name=\"valid\"):\n",
    "        self.model.eval()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (imgs,labels0) in enumerate(tqdm_loader): #,labels1)\n",
    "            with torch.no_grad():\n",
    "                batch_imgs = imgs.cuda().float()[0]\n",
    "                batch_labels0 = labels0.cuda().float()\n",
    "#                 batch_labels1 = labels1.cuda().float()\n",
    "                predicted = self.model(batch_imgs)\n",
    "                loss0 = self.loss_fn(predicted.float().reshape(-1),batch_labels0.float().reshape(-1)).item()\n",
    "#                 loss1 = self.loss_fn(predicted[1].float().reshape(-1),batch_labels1.float().reshape(-1)).item()\n",
    "                loss = loss0 #+ loss1\n",
    "#                 loss = loss/2.0\n",
    "                current_loss_mean = (current_loss_mean * batch_idx + loss) / (batch_idx + 1)\n",
    "                tqdm_loader.set_description(f\"loss : {current_loss_mean:.4}\")\n",
    "            if batch_idx>10:\n",
    "                break\n",
    "        score = 1-current_loss_mean\n",
    "        print('metric {}'.format(score))\n",
    "        return score\n",
    "    \n",
    "    def run(self,train_loder,val_loder):\n",
    "        best_score = -100000\n",
    "        for e in range(self.config.epochs):\n",
    "            print(\"----------Epoch {}-----------\".format(e))\n",
    "            current_loss_mean = self.train_epoch(train_loder)\n",
    "            score = self.valid_epoch(val_loder)\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(self.model.state_dict(),self.config.MODEL_PATH+\"/{}_best.pth\".format(self.config.model_name))\n",
    "\n",
    "    def batch_valid_tta(self, batch_imgs):\n",
    "        batch_imgs = batch_imgs.cuda()\n",
    "        predicted = model(batch_imgs)\n",
    "        tta_flip = [[-1],[-2]]\n",
    "        for axis in tta_flip:\n",
    "            predicted += torch.flip(model(torch.flip(batch_imgs, axis)), axis)\n",
    "        predicted = predicted/(1+len(tta_flip))\n",
    "        predicted = torch.sigmoid(predicted)\n",
    "        return predicted.cpu().numpy()\n",
    "            \n",
    "    def load_best_model(self,):\n",
    "        if os.path.exists(self.config.MODEL_PATH+\"/{}_best.pth\".format(self.config.model_name)):\n",
    "            self.model.load_state_dict(torch.load(self.config.MODEL_PATH+\"/{}_best.pth\".format(self.config.model_name)))\n",
    "            print(\"load best model\")\n",
    "        \n",
    "    def predict(self,imgs_tensor,get_fet = False):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.batch_valid(imgs_tensor,get_fet=get_fet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = trainer(loss_fn,model,optimizer,scheduler,config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=model_config.WORKERS, pin_memory=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=model_config.WORKERS, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.run(train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./saved-models-averagecnnlstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = trainer(loss_fn,model,optimizer,scheduler,config=model_config)\n",
    "mymodel.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, level= next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/xinruizhuang/skin-lesion-classification-acc-90-pytorch\n",
    "# this function is used during training process, to calculation the loss and accuracy\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_loader))\n",
    "image.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()                                                 # assign train mode to the model\n",
    "    \n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()                                     # 과거에 이용한 Mini-Batch내에 있는 이미지 데이터와 레이블 데이터를 바탕으로 계산된 Loss의 Gradient값이 optimizer에 할당되어 있으므로 optimizer의 Gradient 초기화\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()                                           # Back propagation으로 계산된 Gradient 값을 각 parameter에 할당\n",
    "        optimizer.step()                                          # parameter update\n",
    "        scheduler.step()\n",
    "        prediction = output.max(1, keepdim = True)[1]             # predicted labels in tensor\n",
    "        \n",
    "        train_acc.update(prediction.eq(label.view_as(prediction)).sum().item()/BATCH_SIZE)\n",
    "        train_loss.update(loss.item())\n",
    "        \n",
    "        if batch_idx % log_interval == 0:                         # print log\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    total_loss_train.append(train_loss.avg)\n",
    "    total_acc_train.append(train_acc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():                                                     # 모델을 평가하는 단계에서는 Gradient를 통해 parameter값이 update되는 현상을 방지하기 위해 'torch.no_grad()' 메서드를 이용해 Gradient의 흐름을 억제\n",
    "        for image, label in val_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            prediction = output.max(1, keepdim = True)[1]                     # output값 (prediction probability)가 가장 높은 index(class)로 예측\n",
    "    \n",
    "            val_acc.update(prediction.eq(label.view_as(prediction)).sum().item()/BATCH_SIZE)\n",
    "            val_loss.update(loss.item())\n",
    "    \n",
    "    total_acc_val.append(val_acc.avg)\n",
    "    total_loss_val.append(val_loss.avg)\n",
    "    \n",
    "    return val_loss.avg, val_acc.avg   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = 100\n",
    "epoch = 0\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "total_loss_val, total_acc_val = [],[]\n",
    "\n",
    "for Epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer, log_interval = 20)\n",
    "    val_loss, val_acc = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tVal Loss: {:.4f}, \\tVal Accuracy: {:.2f} %\\n\".format(Epoch, val_loss, val_acc))\n",
    "    \n",
    "    # monitoring test accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epoch = Epoch\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (Epoch, val_loss, val_acc))\n",
    "        print('*****************************************************')\n",
    "    elif Epoch > epoch + TRAIN_PATIENCE:\n",
    "        break\n",
    "        \n",
    "#     # monitoring test loss\n",
    "#     if test_loss < best_val_loss:\n",
    "#         best_val_loss = test_loss\n",
    "#         epoch = Epoch\n",
    "#         print('*****************************************************')\n",
    "#         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (Epoch, test_loss, test_accuracy))\n",
    "#         print('*****************************************************')\n",
    "#     elif Epoch > epoch + TRAIN_PATIENCE:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(total_loss_val), max(total_acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.load_best_model()\n",
    "Trainer.predict(image[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num = 2)\n",
    "fig1 = fig.add_subplot(2,1,1)\n",
    "fig2 = fig.add_subplot(2,1,2)\n",
    "fig1.plot(total_loss_train, label = 'training loss')\n",
    "fig1.plot(total_loss_val, label = 'test loss')\n",
    "fig2.plot(total_acc_train, label = 'training accuracy')\n",
    "fig2.plot(total_acc_val, label = 'test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[:,0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet3d = torchvision.models.video.r3d_18(pretrained = True, progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier layer output shape --> number of classes\n",
    "CLASSES = 3\n",
    "\n",
    "num_ftrs = resnet3d.fc.in_features\n",
    "resnet3d.fc = nn.Linear(num_ftrs,CLASSES)\n",
    "resnet3d.fc = nn.Sequential(resnet3d.fc, nn.Softmax(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = resnet3d.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_dataset[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = resnet3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = WORK_DIR + 'saved-models/'\n",
    "\n",
    "weights = os.listdir(DIR)\n",
    "weights.sort()\n",
    "# val_losses = [filename.split('-')[2].split('val')[1] for filename in weights]\n",
    "val_losses = [filename.split('-')[2].split('val')[1] for filename in weights] #[1:]]\n",
    "best_weight = val_losses.index(min(val_losses))\n",
    "# path = './saved_models/' + weights[best_weight]\n",
    "path = DIR + weights[best_weight] #+1] # '.ipynb_checkpoints' 파일이 맨 앞에 있기 때문에 index가 하나씩 밀린다.\n",
    "\n",
    "# best_model = torch.load(DIR+'model-epoch00-loss_val1.08-acc_val0.46.pt')\n",
    "checkpoint = torch.load(path)\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "accuracy = checkpoint['accuracy']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/dataset/test/annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /ainode/dataset/test/flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls /ainode/dataset/test/rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "predicted_values = []\n",
    "\n",
    "# best_model.eval()\n",
    "    \n",
    "test_loss = AverageMeter()\n",
    "test_acc = AverageMeter()\n",
    "\n",
    "with torch.no_grad():                                                     # 모델을 평가하는 단계에서는 Gradient를 통해 parameter값이 update되는 현상을 방지하기 위해 'torch.no_grad()' 메서드를 이용해 Gradient의 흐름을 억제\n",
    "    for image, label in test_loader:\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        output = best_model(image)\n",
    "        loss = criterion(output, label)\n",
    "        prediction = output.max(1, keepdim = True)[1]                     # output값 (prediction probability)가 가장 높은 index(class)로 예측\n",
    "        \n",
    "        true_labels.append(label)\n",
    "        predicted_labels.append(prediction)\n",
    "        predicted_values.append(output)\n",
    "        \n",
    "        test_acc.update(prediction.eq(label.view_as(prediction)).sum().item()/BATCH_SIZE)\n",
    "        test_loss.update(loss.item())\n",
    "        \n",
    "test_loss.avg, test_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to array form\n",
    "true_test_lst = []\n",
    "predicted_test_lst = []\n",
    "predicted_value_lst = []\n",
    "\n",
    "for i in range(len(test_loader)):\n",
    "    t_labels = true_labels[i].cpu().numpy()\n",
    "    true_test_lst += list(t_labels)\n",
    "    \n",
    "    p_labels = predicted_labels[i].cpu().numpy()[:,0]\n",
    "    predicted_test_lst += list(p_labels)\n",
    "    \n",
    "    p_values = predicted_values[i].cpu().numpy()\n",
    "    predicted_value_lst += list(p_values)\n",
    "    \n",
    "true_test_arr = np.array(true_test_lst)\n",
    "predicted_test_arr = np.array(predicted_test_lst)\n",
    "predicted_value_arr = np.array([list(arr) for arr in predicted_value_lst])\n",
    "    \n",
    "true_test_arr[:20], predicted_test_arr[:20], predicted_value_arr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "tick_labels = ['Wake', 'Light Sleep','Deep Sleep']\n",
    "\n",
    "cm = confusion_matrix(true_test_arr, predicted_test_arr)\n",
    "ax = sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "ax.set_xticklabels(tick_labels)\n",
    "ax.set_yticklabels(tick_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "labels = ['Wake', 'Light Sleep','Deep Sleep']\n",
    "\n",
    "print(classification_report(true_test_arr, predicted_test_arr, target_names = labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve for classes\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 3\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(true_test_arr == i, predicted_value_arr[:, i])\n",
    "    \n",
    "# plotting\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='blue', label='Positive: Opacity / Negative: Normal') # Sensitivity: 비정상 중에 비정상으로 predict된 비율 / Specificity: 정상 중에 정상으로 predict된 비율\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend(loc='best') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
